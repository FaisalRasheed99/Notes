{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOpQpXhBjC93"
      },
      "source": [
        "# RAG System with Feedback Loop: Enhancing Retrieval and Response Quality\n",
        "\n",
        "## Overview\n",
        "\n",
        "This system implements a Retrieval-Augmented Generation (RAG) approach with an integrated feedback loop. It aims to improve the quality and relevance of responses over time by incorporating user feedback and dynamically adjusting the retrieval process.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional RAG systems can sometimes produce inconsistent or irrelevant responses due to limitations in the retrieval process or the underlying knowledge base. By implementing a feedback loop, we can:\n",
        "\n",
        "1. Continuously improve the quality of retrieved documents\n",
        "2. Enhance the relevance of generated responses\n",
        "3. Adapt the system to user preferences and needs over time\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **PDF Content Extraction**: Extracts text from PDF documents\n",
        "2. **Vectorstore**: Stores and indexes document embeddings for efficient retrieval\n",
        "3. **Retriever**: Fetches relevant documents based on user queries\n",
        "4. **Language Model**: Generates responses using retrieved documents\n",
        "5. **Feedback Collection**: Gathers user feedback on response quality and relevance\n",
        "6. **Feedback Storage**: Persists user feedback for future use\n",
        "7. **Relevance Score Adjustment**: Modifies document relevance based on feedback\n",
        "8. **Index Fine-tuning**: Periodically updates the vectorstore using accumulated feedback\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### 1. Initial Setup\n",
        "- The system reads PDF content and creates a vectorstore\n",
        "- A retriever is initialized using the vectorstore\n",
        "- A language model (LLM) is set up for response generation\n",
        "\n",
        "### 2. Query Processing\n",
        "- When a user submits a query, the retriever fetches relevant documents\n",
        "- The LLM generates a response based on the retrieved documents\n",
        "\n",
        "### 3. Feedback Collection\n",
        "- The system collects user feedback on the response's relevance and quality\n",
        "- Feedback is stored in a JSON file for persistence\n",
        "\n",
        "### 4. Relevance Score Adjustment\n",
        "- For subsequent queries, the system loads previous feedback\n",
        "- An LLM evaluates the relevance of past feedback to the current query\n",
        "- Document relevance scores are adjusted based on this evaluation\n",
        "\n",
        "### 5. Retriever Update\n",
        "- The retriever is updated with the adjusted document scores\n",
        "- This ensures that future retrievals benefit from past feedback\n",
        "\n",
        "### 6. Periodic Index Fine-tuning\n",
        "- At regular intervals, the system fine-tunes the index\n",
        "- High-quality feedback is used to create additional documents\n",
        "- The vectorstore is updated with these new documents, improving overall retrieval quality\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. **Continuous Improvement**: The system learns from each interaction, gradually enhancing its performance.\n",
        "2. **Personalization**: By incorporating user feedback, the system can adapt to individual or group preferences over time.\n",
        "3. **Increased Relevance**: The feedback loop helps prioritize more relevant documents in future retrievals.\n",
        "4. **Quality Control**: Low-quality or irrelevant responses are less likely to be repeated as the system evolves.\n",
        "5. **Adaptability**: The system can adjust to changes in user needs or document contents over time.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This RAG system with a feedback loop represents a significant advancement over traditional RAG implementations. By continuously learning from user interactions, it offers a more dynamic, adaptive, and user-centric approach to information retrieval and response generation. This system is particularly valuable in domains where information accuracy and relevance are critical, and where user needs may evolve over time.\n",
        "\n",
        "While the implementation adds complexity compared to a basic RAG system, the benefits in terms of response quality and user satisfaction make it a worthwhile investment for applications requiring high-quality, context-aware information retrieval and generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4R35uOzjC97"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/retrieval_with_feedback_loop.svg?raw=1\" alt=\"retrieval with feedback loop\" style=\"width:40%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45OhOfsHjC98"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yLNAMgbgjC99"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "#!pip install langchain langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "idujYXISV6Wy"
      },
      "outputs": [],
      "source": [
        "#pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lg-fJb9LDpG3"
      },
      "outputs": [],
      "source": [
        "#pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CpmZJZFutvIt"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain-community==0.2.15  langchain-text-splitters==0.2.2 langchain-huggingface==0.0.3 \"transformers>=4.41.0\" faiss-cpu langchain-groq==0.1.9 PyPDF2-3.0.1 unstructured==0.15.0 unstructured[pdf]==0.15.0 nltk==3.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd8Vy-Y3ElGE",
        "outputId": "299aaf56-2f55-461e-b29c-b25d8acd3be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.7)\n"
          ]
        }
      ],
      "source": [
        "#pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H2zdP4gwtO7O"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain.prompts import PromptTemplate\n",
        "import openai\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "import requests\n",
        "import google.generativeai as genai\n",
        "from langchain_core.language_models import LLM\n",
        "from typing import Optional, List\n",
        "from PyPDF2 import PdfFileReader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "import time\n",
        "import json\n",
        "from typing import List, Optional\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "#from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from typing import List, Dict, Any\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_groq import ChatGroq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drm1LwDwmFSW"
      },
      "source": [
        "**Load llm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlGPtqndmAgJ"
      },
      "outputs": [],
      "source": [
        "llm = ChatGroq(\n",
        "    api_key=\"API_KEY\",\n",
        "    model_name=\"llama-3.1-8b-instant\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms1CPIy8jC9-"
      },
      "source": [
        "### Define documents path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tIroYUNjC9-",
        "outputId": "ce2c658b-d19d-4abb-becc-a20f9b7bccd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-14 10:45:10--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "\r          data/Unde   0%[                    ]       0  --.-KB/s               \rdata/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-12-14 10:45:10 (10.9 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n",
            "--2025-12-14 10:45:10--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/feedback_data.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-12-14 10:45:10 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/feedback_data.json https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/feedback_data.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSLjMgN8smRY"
      },
      "source": [
        "### Extract text from the PDF File."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hQK1osqwrFWE"
      },
      "outputs": [],
      "source": [
        "import pymupdf\n",
        "def read_pdf_to_string(path):\n",
        "    \"\"\"\n",
        "    Read a PDF document from the specified path and return its content as a string.\n",
        "\n",
        "    Args:\n",
        "    path (str): The file path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "    str: The concatenated text content of all pages in the PDF document.\n",
        "\n",
        "    The function uses the 'fitz' library (PyMuPDF) to open the PDF document, iterate over each page,\n",
        "    extract the text content from each page, and append it to a single string.\"\"\"\n",
        "\n",
        "    # Open the PDF document located at the specified path\n",
        "    doc = pymupdf.open(path)\n",
        "    content = \"\"\n",
        "    # Iterate over each page in the document\n",
        "    for page_num in range(len(doc)):\n",
        "        # Get the current page\n",
        "        page = doc[page_num]\n",
        "        # Extract the text content from the current page and append it to the content string\n",
        "        content += page.get_text()\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhgqdSCiylGu"
      },
      "source": [
        "Loading Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7xq9n9HykMl",
        "outputId": "188d1952-901f-4a41-c9a5-a093cf1458e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1699994654.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJSAcDNnjC9_"
      },
      "source": [
        "### Create vector store and retrieval QA chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "et6SHiybyeiH"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "def encode_from_string(content, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a string into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "    content (str): The text content to be encoded.\n",
        "    chunk_size (int): The size of each chunk of text.\n",
        "    chunk_overlap (int): The overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "    FAISS: A vector store containing the encoded content.\n",
        "\n",
        "    Raises:\n",
        "    ValueError: If the input content is not valid.\n",
        "    RuntimeError: If there is an error during the encoding process.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(content, str) or not content.strip():\n",
        "      raise ValueError(\"Content must be a non-empty string.\")\n",
        "\n",
        "    if not isinstance(chunk_size, int) or chunk_size <= 0:\n",
        "      raise ValueError(\"chunk_size must be a positive integer.\")\n",
        "\n",
        "    if not isinstance(chunk_overlap, int) or chunk_overlap < 0:\n",
        "      raise ValueError(\"chunk_overlap must be a non-negative integer.\")\n",
        "\n",
        "    try:\n",
        "    # Split the content into chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "        )\n",
        "        chunks = text_splitter.create_documents([content])\n",
        "\n",
        "        # Assign metadata to each chunk\n",
        "        for chunk in chunks:\n",
        "            chunk.metadata['relevance_score'] = 1.0\n",
        "\n",
        "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "        return vectorstore\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"An error occured during the encoding process: {str(e)}\")\n",
        "\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AXdnp_N_HuEq"
      },
      "outputs": [],
      "source": [
        "path = \"/content/item.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CJ_ByaEOjC9_"
      },
      "outputs": [],
      "source": [
        "content = read_pdf_to_string(path)\n",
        "vectorstore = encode_from_string(content)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3aUSnTdjC-A"
      },
      "source": [
        "### Function to format user feedback in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_k9bw5qQjC-A"
      },
      "outputs": [],
      "source": [
        "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": response,\n",
        "        \"relevance\": int(relevance),\n",
        "        \"quality\": int(quality),\n",
        "        \"comments\": comments\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yGhWuWpjC-A"
      },
      "source": [
        "### Function to store the feedback in a json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LgU3nWYMjC-A"
      },
      "outputs": [],
      "source": [
        "def store_feedback(feedback):\n",
        "    with open(\"/content/feedback_data.json\", \"a\") as f:\n",
        "        json.dump(feedback, f)\n",
        "        f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EapPdfwzjC-A"
      },
      "source": [
        "### Function to read the feedback file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DPFoQzuCjC-B"
      },
      "outputs": [],
      "source": [
        "def load_feedback_data():\n",
        "    feedback_data = []\n",
        "    try:\n",
        "        with open(\"/content/feedback_data.json\", \"r\") as f:\n",
        "        #with open(\"data/feedback_data.json\", \"r\") as f:\n",
        "            for line in f:\n",
        "                feedback_data.append(json.loads(line.strip()))\n",
        "    except FileNotFoundError:\n",
        "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
        "    return feedback_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_BUD7WJjC-B"
      },
      "source": [
        "### Function to adjust files relevancy based on the feedbacks file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mDmbfwFcjC-B"
      },
      "outputs": [],
      "source": [
        "# class Response(BaseModel):\n",
        "#     answer: str = Field(..., title=\"The answer to the question. The options can be only 'Yes' or 'No'\")\n",
        "\n",
        "def adjust_relevance_scores(query: str, docs: List[Any], feedback_data: List[Dict[str, Any]]) -> List[Any]:\n",
        "    # Create a prompt template for relevance checking\n",
        "    relevance_prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"feedback_query\", \"doc_content\", \"feedback_response\"],\n",
        "        template=\"\"\"\n",
        "        Determine if the following feedback response is relevant to the current query and document content.\n",
        "        You are also provided with the Feedback original query that was used to generate the feedback response.\n",
        "        Current query: {query}\n",
        "        Feedback query: {feedback_query}\n",
        "        Document content: {doc_content}\n",
        "        Feedback response: {feedback_response}\n",
        "\n",
        "        Is this feedback relevant? Respond with only 'Yes' or 'No'.\n",
        "        \"\"\"\n",
        "    )\n",
        "    #llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
        "\n",
        "    # Create an LLMChain for relevance checking\n",
        "    #relevance_chain = relevance_prompt | llm.with_structured_output(Response)\n",
        "    relevance_chain = LLMChain(prompt = relevance_prompt, llm=llm)\n",
        "\n",
        "    for doc in docs:\n",
        "        relevant_feedback = []\n",
        "\n",
        "        for feedback in feedback_data:\n",
        "            # Use LLM to check relevance\n",
        "            input_data = {\n",
        "                \"query\": query,\n",
        "                \"feedback_query\": feedback['query'],\n",
        "                \"doc_content\": doc.page_content[:1000],\n",
        "                \"feedback_response\": feedback['response']\n",
        "            }\n",
        "            #result = relevance_chain.invoke(input_data).answer\n",
        "            result = relevance_chain.run(input_data)\n",
        "\n",
        "            if result == 'yes':\n",
        "                relevant_feedback.append(feedback)\n",
        "\n",
        "        # Adjust the relevance score based on feedback\n",
        "        if relevant_feedback:\n",
        "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
        "            doc.metadata['relevance_score'] *= (avg_relevance / 3)  # Assuming a 1-5 scale, 3 is neutral\n",
        "\n",
        "    # Re-rank documents based on adjusted scores\n",
        "    return sorted(docs, key=lambda x: x.metadata['relevance_score'], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrrC4ZGpjC-B"
      },
      "source": [
        "### Function to fine tune the vector index to include also queries + answers that received good feedbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wdwFdCnujC-B"
      },
      "outputs": [],
      "source": [
        "def fine_tune_index(feedback_data: List[Dict[str, Any]], texts: List[str]) -> Any:\n",
        "    # Filter high-quality responses\n",
        "    good_responses = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
        "\n",
        "    # Extract queries and responses, and create new documents\n",
        "    additional_texts = []\n",
        "    for f in good_responses:\n",
        "        combined_text = f['query'] + \" \" + f['response']\n",
        "        additional_texts.append(combined_text)\n",
        "\n",
        "    # make the list a string\n",
        "    additional_texts = \" \".join(additional_texts)\n",
        "\n",
        "    # Create a new index with original and high-quality texts\n",
        "    all_texts = texts + additional_texts\n",
        "    new_vectorstore = encode_from_string(all_texts)\n",
        "\n",
        "    return new_vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0KbkbukASQ8"
      },
      "source": [
        "### Update the retriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "86znK9tmN6Kc"
      },
      "outputs": [],
      "source": [
        "def retrieval_update(retriever):\n",
        "    # Adjust relevance scores for future retrievals\n",
        "    #global retriever\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    adjusted_docs = adjust_relevance_scores(query, docs, load_feedback_data())\n",
        "    # Update the retriever with adjusted docs\n",
        "    retriever.search_kwargs['k'] = len(adjusted_docs)\n",
        "    retriever.search_kwargs['docs'] = adjusted_docs\n",
        "    # Periodically (e.g., daily or weekly), fine-tune the index\n",
        "    new_vectorstore = fine_tune_index(load_feedback_data(), content)\n",
        "    retriever = new_vectorstore.as_retriever()\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
        "    return retriever, qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw_vxnBpjC-C"
      },
      "source": [
        "### Demonstration of how to retrieve answers with respect to user feedbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwz5xcunPvgt",
        "outputId": "683183b4-cb98-4080-be1b-9a9fdc41a046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Object `#from langchain import PromptTemplate` not found.\n",
            "Enter your query: How many layers are there in the atmosphere?\n",
            "\n",
            "Response: The text doesn't explicitly mention the number of layers in the atmosphere, but it does mention the \"troposphere\" which is a specific layer. However, I can provide general information about the layers of the atmosphere.\n",
            "\n",
            "The Earth's atmosphere is generally divided into 5 layers:\n",
            "\n",
            "1. Troposphere: The lowest layer, extending up to about 12 km (7.5 miles) above the Earth's surface.\n",
            "2. Stratosphere: Above the troposphere, extending up to about 50 km (31 miles).\n",
            "3. Mesosphere: Above the stratosphere, extending up to about 85 km (53 miles).\n",
            "4. Thermosphere: Above the mesosphere, extending up to about 600 km (373 miles).\n",
            "5. Exosphere: The outermost layer, extending from the thermosphere to the edge of space, where the atmosphere interacts with the solar wind.\n",
            "\n",
            "Please note that these layers are not sharply defined and can overlap in certain regions.\n",
            "\n",
            "Are you satisfied with the result? (yes/no): no\n",
            "\n",
            "What would you like to do? \n",
            "1. Give feedback rating\n",
            "2. Provide your own answer\n",
            "Enter 1 or 2: 2\n",
            "Please provide your own answer: Can you confirm that its really 5 or 6?\n",
            "Thank you for your input!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2056456356.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
            "  docs = retriever.get_relevant_documents(query)\n",
            "/tmp/ipython-input-2024395955.py:23: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  relevance_chain = LLMChain(prompt = relevance_prompt, llm=llm)\n",
            "/tmp/ipython-input-2024395955.py:37: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  result = relevance_chain.run(input_data)\n"
          ]
        }
      ],
      "source": [
        "#from langchain import PromptTemplate\n",
        "\n",
        "while True:\n",
        "    query = input(\"Enter your query: \")\n",
        "    response = qa_chain.invoke({\"query\": query})[\"result\"]\n",
        "    print(\"\\nResponse:\", response)\n",
        "\n",
        "    satisfaction = input(\"\\nAre you satisfied with the result? (yes/no): \").strip().lower()\n",
        "\n",
        "    if satisfaction == \"yes\":\n",
        "        print(\"Thank you for your feedback!\")\n",
        "        break\n",
        "    elif satisfaction == \"no\":\n",
        "        action = input(\"\\nWhat would you like to do? \\n1. Give feedback rating\\n2. Provide your own answer\\nEnter 1 or 2: \").strip()\n",
        "\n",
        "        if action == \"1\":\n",
        "            relevance = input(\"Rate the relevance from 1 to 5: \")\n",
        "            print(\"Thank you for your rating!\")\n",
        "            quality = input(\"Rate the quality from 1 to 5: \")\n",
        "            print(\"Thank you for your rating!\")\n",
        "            # Collect feedback\n",
        "            feedback = get_user_feedback(query, response, relevance, quality)\n",
        "            # Store feedback\n",
        "            store_feedback(feedback)\n",
        "\n",
        "            retriever, qa_chain = retrieval_update(retriever)\n",
        "\n",
        "        elif action == \"2\":\n",
        "            user_answer = input(\"Please provide your own answer: \")\n",
        "            print(\"Thank you for your input!\")\n",
        "            relevance = 5\n",
        "            quality = 5\n",
        "            # Collect feedback\n",
        "            feedback = get_user_feedback(query, user_answer, relevance, quality)\n",
        "            # Store feedback\n",
        "            store_feedback(feedback)\n",
        "            retriever, qa_chain = retrieval_update(retriever)\n",
        "        else:\n",
        "              print(\"Invalid choice. Exiting.\")\n",
        "        break\n",
        "    else:\n",
        "          print(\"Invalid input. Please enter yes or no.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G-t3vdAjC-C"
      },
      "source": [
        "### Finetune the vectorstore periodicly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dkfu9LpfjC-C"
      },
      "outputs": [],
      "source": [
        "# Periodically (e.g., daily or weekly), fine-tune the index\n",
        "# new_vectorstore = fine_tune_index(load_feedback_data(), content)\n",
        "# retriever = new_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxAGZ_AGjC-C"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--retrieval-with-feedback-loop)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
